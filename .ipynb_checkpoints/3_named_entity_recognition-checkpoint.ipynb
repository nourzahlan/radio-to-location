{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#! python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re\n",
    "import pandas as pd\n",
    "import en_core_web_sm as en\n",
    "from spacy import displacy\n",
    "from spacy.attrs import LOWER \n",
    "from collections import Counter\n",
    "from spacy.matcher import Matcher\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dataframes from csv\n",
    "df = pd.read_csv('./datasets/radio.csv')\n",
    "df_context = pd.read_csv('./datasets/radio_context.csv')\n",
    "\n",
    "df_roads = pd.read_csv('./datasets/roads.csv')\n",
    "list_of_roads = list(df_roads['road_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[spaCy](https://spacy.io/) finds entities in a document by tokenizing the strings and then assigning each word a tag. It then looks for patterns to get entities and classifies them with labels. Here we used spaCy to extract entities with labels related to locations (GPE and FAC) to be able to extract them from our transcripts. GPE is the acronym for \"geo-political entities\" and FAC, \"facility\", which locates airports, highways, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to extract locations using spaCy pre trained labels\n",
    "def location_extraction(string_in):\n",
    "    doc = nlp(string_in)\n",
    "    locations = []\n",
    "    # loop through every entity in the transcript\n",
    "    for X in doc.ents:\n",
    "        if (X.label_ == 'FAC') or (X.label_ == 'GPE'):\n",
    "            locations.append(X.text)\n",
    "    if len(locations) != 0:\n",
    "        return locations\n",
    "    return None\n",
    "\n",
    "# Add a column with the extracted locations\n",
    "df['location_extraction'] = df['transcripts'].map(location_extraction)\n",
    "\n",
    "# Since we dont care about transcripts where we didn't find any locations we drop all NAs\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  spaCy Matcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use spaCy Matcher entity to be able to generate our own set of rules to look for in the text. Every rule corresponds to patterns which consists of sets of words, conditions and operators, where the word had to be found in the document following a specific condition and the operator determines how many times or how we have to observe the pattern.\n",
    "Here we are looking for entities that correspond to a road name in Butte county, and since we already have a complete list of road names, we can set one rule for each road, where the pattern would specify and all words have to match exactly one time except if the name ended with a generic word like \"street\" or \"Road\" then that word could match 0 or more times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the Matcher entity\n",
    "\n",
    "# Instantiate\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# specifies what spacy does when it finds a match in the document. Here we just want to return the matches\n",
    "def on_match(matcher, doc, id, matches):\n",
    "    return matches\n",
    "\n",
    "# building patterns for every road name, the condition being that the lowercase entity in the doc should match \n",
    "# the lowercase verion of the road name, so that capitalization wouldn't affect the model\n",
    "def build_pattern(road_name):\n",
    "    list_words = road_name.split(' ')\n",
    "    # general words that appear a lot in the list. \n",
    "    # The reason why we do this is to still get a match if they are not present\n",
    "    roads_general = ['lane', 'road', \n",
    "                 'court', 'drive', \n",
    "                 'avenue', 'way', \n",
    "                 'street', 'circle', \n",
    "                 'place', 'highway', 'trail']\n",
    "    if list_words[-1].lower() in roads_general:\n",
    "        pattern = [{'LOWER': word.lower()} for word in list_words[:-1]]\n",
    "        pattern.append({'op': '*', 'LOWER' : list_words[-1].lower()})\n",
    "    else:\n",
    "        pattern = [{'LOWER': word.lower()} for word in list_words]\n",
    "    return pattern\n",
    "\n",
    "# Get a pattern of every road\n",
    "for road in list_of_roads:\n",
    "    matcher.add(road, on_match, build_pattern(road))\n",
    "    \n",
    "# This function takes a string as input and returns it with every word capitalized\n",
    "def capitalize_string(string_in):\n",
    "    words = string_in.split(' ')\n",
    "    string_out = ''\n",
    "    for i in words:\n",
    "        string_out += i.capitalize() + ' '\n",
    "    string_out = string_out[:-1]\n",
    "    return string_out   \n",
    "    \n",
    "# Look for locations in the transcript, then extract them\n",
    "def location_extraction_context(string_in):\n",
    "    doc = nlp(string_in)\n",
    "    string_out = ''\n",
    "    list_words = string_in.split(' ')\n",
    "    matches = matcher(doc)\n",
    "    if len(matches) == 0:\n",
    "        return None\n",
    "    indeces_to_pop = []\n",
    "    # loop through the matches and delete those that are a subset of another. \n",
    "    # this was done because some road names have words in commond and we were getting 2 matches for some locations\n",
    "    # here we eliminate the shorter one since the longest is clearly the one intended\n",
    "    for a in range(len(matches)):\n",
    "        for b in range(a+1, len(matches)):\n",
    "            if (matches[a][2] == matches[b][2]):\n",
    "                if (matches[a][1] < matches[b][1]):\n",
    "                    indeces_to_pop.append(b)\n",
    "                else:\n",
    "                    indeces_to_pop.append(a)\n",
    "    matches_final = [tup for index, tup in enumerate(matches) if index not in indeces_to_pop]\n",
    "    # loop through the matches and add them to the string to return\n",
    "    # matches consist of an id and the indeces of the first and last word that constitute the pattern in the document\n",
    "    # we use the ids to extrat the locations from the rules in the Matcher instance and not from the text itself,\n",
    "    # to make sure they all follow the same format\n",
    "    for match in matches_final:\n",
    "        list_pattern = matcher.get(match[0])[1][0]\n",
    "        for token in list_pattern:\n",
    "            string_out += token['LOWER'] + ' '\n",
    "        string_out += ', '\n",
    "    string_out = string_out[:-3]\n",
    "    string_out = capitalize_string(string_out)\n",
    "    return string_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to remove stopwords from a string\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "def remove_stopwords(string_in):\n",
    "    \n",
    "    word_tokens = word_tokenize(string_in)\n",
    "    filtered_words = [w for w in word_tokens if not w in stop_words] \n",
    "    filtered_string = ''\n",
    "    for word in filtered_words:\n",
    "        filtered_string += word + ' '\n",
    "    return filtered_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column with transcripts without stopwords\n",
    "df_context['transcripts_no_stopwords'] = df_context['transcripts'].map(remove_stopwords)\n",
    "\n",
    "# Add a column with the extracted locations\n",
    "df_context['location_extraction'] = df_context['transcripts_no_stopwords'].map(location_extraction_context)\n",
    "\n",
    "# Since we dont care about transcripts where we didn't find any locations we drop all NAs\n",
    "df_context.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save them as csv\n",
    "df.to_csv('./datasets/location.csv')\n",
    "df_context.to_csv('./datasets/location_context.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
